{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "model.init_sims(replace=True) # The vectors shall be normalized by L2 norm to reproduce the results published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_full_list(counterpart, obj, subj):\n",
    "    '''\n",
    "    This function aims at finding the 10 closest matches for analogies like\n",
    "    \n",
    "    vec(he) - vec(king) ~= vec(she) - vec(??)\n",
    "    \n",
    "    where ?? is queen. This is an extension to the `most_similar` method of the class \n",
    "    gensim.models.keyedvectors.WordEmbeddingsKeyedVectors that it finds the closest\n",
    "    matches of vector representation to\n",
    "    \n",
    "    vec(she) - vec(he) + vec(king),\n",
    "    \n",
    "    and vec(king) is also included in the returned list for comparison.\n",
    "    \n",
    "    Args:\n",
    "        counterpart (str): The counterpart of the subject in the analogy (\"she\" above)\n",
    "        obj (str): The object of which its relative relationship with subject is of interest.\n",
    "                   In the above example, it is \"king\"\n",
    "        subj (str): The subject with an object, which is \"he\" in the above example\n",
    "\n",
    "    Returns:\n",
    "        A list of 11 tuples indicating vectors with highest cosine similarity with the vector\n",
    "        vec(counterpart) - vec(subj) + vec(obj)\n",
    "\n",
    "    '''\n",
    "    top_ten = model.most_similar(positive=[counterpart, obj], negative=[subj])\n",
    "    vec1 = (model.get_vector(counterpart) + model.get_vector(obj) - model.get_vector(subj)).reshape(1, 300)\n",
    "    vec2 = model.get_vector(obj).reshape(1, 300)\n",
    "    top_ten.append((obj, cosine_similarity(vec1, vec2)[0][0]))\n",
    "    return sorted(top_ten, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demostration of how word-embedding can give analogy in Mikolov *et al*. (2013)\n",
    "\n",
    "$\\vec{\\text{Paris}} - \\vec{\\text{France}} \\approx \\vec{\\text{Tokyo}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Japan', 0.8167769908905029),\n",
       " ('Japanese', 0.648090124130249),\n",
       " ('South_Korea', 0.6141558885574341),\n",
       " ('Japans', 0.6117385029792786),\n",
       " ('Shizuoka', 0.5742497444152832),\n",
       " ('Aomori_Prefecture', 0.5598059892654419),\n",
       " ('northernmost_prefecture', 0.5524747967720032),\n",
       " ('Kyushu', 0.5514252185821533),\n",
       " ('captain_Makoto_Hasebe', 0.5508174896240234),\n",
       " ('Shimane', 0.5497493743896484),\n",
       " ('France', 0.53038573)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(counterpart='Tokyo', obj='France', subj='Paris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An appropriate *she-he* analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{\\text{he}} - \\vec{\\text{king}} \\approx \\vec{\\text{she}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7633836269378662),\n",
       " ('king', 0.6967242),\n",
       " ('princess', 0.6342117786407471),\n",
       " ('queens', 0.5744965076446533),\n",
       " ('monarch', 0.5577754974365234),\n",
       " ('goddess', 0.5278830528259277),\n",
       " ('princesses', 0.5202734470367432),\n",
       " ('Queen_Consort', 0.5134546756744385),\n",
       " ('very_pampered_McElhatton', 0.5131746530532837),\n",
       " ('empress', 0.5119600892066956),\n",
       " ('queendom', 0.5091063380241394)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list( counterpart='she', obj='king', subj='he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{\\text{he}} - \\vec{\\text{brother}} \\approx \\vec{\\text{she}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister', 0.7946048378944397),\n",
       " ('daughter', 0.7510346174240112),\n",
       " ('mother', 0.7232228517532349),\n",
       " ('brother', 0.70276976),\n",
       " ('husband', 0.702451765537262),\n",
       " ('niece', 0.6873900890350342),\n",
       " ('aunt', 0.661358118057251),\n",
       " ('eldest_daughter', 0.6575361490249634),\n",
       " ('sisters', 0.6565696597099304),\n",
       " ('twin_sister', 0.635208010673523),\n",
       " ('neice', 0.6324375867843628)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list( counterpart='she', obj='brother', subj='he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of gender stereotype *she-he* analogies mentioned in Bolukbasi *et al*. (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *sewing-carpentry*\n",
    "\n",
    "$\\vec{\\text{she}} - \\vec{\\text{sewing}} \\approx \\vec{\\text{he}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sewing', 0.66992193),\n",
       " ('woodworking', 0.5794616937637329),\n",
       " ('sew', 0.531487226486206),\n",
       " ('carpentry', 0.5230334997177124),\n",
       " ('woodcarving', 0.49166664481163025),\n",
       " ('wood_carving', 0.4753499925136566),\n",
       " ('leatherworking', 0.4700630009174347),\n",
       " ('Sewing', 0.4634256660938263),\n",
       " ('knitting', 0.46304479241371155),\n",
       " ('spinning_weaving', 0.4606916308403015),\n",
       " ('woodworking_shop', 0.45343706011772156)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='sewing', counterpart='he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{\\text{he}} - \\vec{\\text{sewing}} \\approx \\vec{\\text{she}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sewing', 0.8904124),\n",
       " ('knitting', 0.6953117251396179),\n",
       " ('quilting', 0.6564728021621704),\n",
       " ('crocheting', 0.6488434076309204),\n",
       " ('Sewing', 0.6432007551193237),\n",
       " ('sew', 0.6409136652946472),\n",
       " ('needlework', 0.6230053305625916),\n",
       " ('sewing_embroidery', 0.6086386442184448),\n",
       " ('crochet', 0.6023696064949036),\n",
       " ('embroidery', 0.6006345152854919),\n",
       " ('sewing_machine', 0.5801945924758911)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='he', obj='sewing', counterpart='she')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *nurse-surgeon*\n",
    "\n",
    "$\\vec{\\text{she}} - \\vec{\\text{nurse}} \\approx \\vec{\\text{he}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nurse', 0.6655272),\n",
       " ('doctor', 0.5559605360031128),\n",
       " ('medic', 0.5425376892089844),\n",
       " ('physician', 0.5394270420074463),\n",
       " ('x_ray_technician', 0.5355567932128906),\n",
       " ('surgeon', 0.516014575958252),\n",
       " ('nurses', 0.49741458892822266),\n",
       " ('paramedic', 0.4924110770225525),\n",
       " ('anesthetist', 0.4886544942855835),\n",
       " ('patient', 0.46544167399406433),\n",
       " ('doctors', 0.4639861583709717)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='nurse', counterpart='he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{\\text{she}} - \\vec{\\text{nurse}} \\approx \\vec{\\text{she}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nurse', 1.0),\n",
       " ('registered_nurse', 0.7907712459564209),\n",
       " ('nurses', 0.738167405128479),\n",
       " ('nurse_practitioner', 0.699310302734375),\n",
       " ('midwife', 0.6727651953697205),\n",
       " ('respiratory_therapist', 0.6620449423789978),\n",
       " ('Nurse', 0.6428854465484619),\n",
       " ('nursing', 0.6424654722213745),\n",
       " ('doctor', 0.6319522857666016),\n",
       " ('neonatal_nurse', 0.6193329691886902),\n",
       " ('x_ray_technician', 0.6126095056533813)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='nurse', counterpart='she')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{\\text{he}} - \\vec{\\text{nurse}} \\approx \\vec{\\text{she}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nurse', 0.82805425),\n",
       " ('registered_nurse', 0.7027999758720398),\n",
       " ('nurse_practitioner', 0.6314352750778198),\n",
       " ('midwife', 0.6205434203147888),\n",
       " ('nurses', 0.6066274046897888),\n",
       " ('certified_lactation_counselor', 0.5820688605308533),\n",
       " ('nurse_midwife', 0.5799018144607544),\n",
       " ('birth_doula', 0.5744963884353638),\n",
       " ('neonatal_nurse', 0.5663833022117615),\n",
       " ('dental_hygienist', 0.551973819732666),\n",
       " ('lactation_consultant', 0.5466617345809937)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='he', obj='nurse', counterpart='she')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{\\text{he}} - \\vec{\\text{surgeon}} \\approx \\vec{\\text{she}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('surgeon', 0.723608),\n",
       " ('gynecologist', 0.6337347030639648),\n",
       " ('surgeons', 0.6173180341720581),\n",
       " ('plastic_surgeon', 0.5985187888145447),\n",
       " ('nurse', 0.5903362035751343),\n",
       " ('cosmetic_surgeon', 0.5817878246307373),\n",
       " ('hysterectomy', 0.5799472332000732),\n",
       " ('obstetrician', 0.5660049319267273),\n",
       " ('sonographer', 0.5639315843582153),\n",
       " ('midwife', 0.5569908618927002),\n",
       " ('MRI_technologist', 0.546063244342804)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='he', obj='surgeon', counterpart='she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('surgeon', 1.0000001),\n",
       " ('surgeons', 0.7731136083602905),\n",
       " ('neurosurgeon', 0.7661304473876953),\n",
       " ('Surgeon', 0.7248443365097046),\n",
       " ('orthopedic_surgeon', 0.7162246704101562),\n",
       " ('plastic_surgeon', 0.70467209815979),\n",
       " ('doctor', 0.6793397665023804),\n",
       " ('urologist', 0.6659057140350342),\n",
       " ('vascular_surgeon', 0.6651214361190796),\n",
       " ('thoracic_surgeon', 0.6579000949859619),\n",
       " ('physician', 0.6561126708984375)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='he', obj='surgeon', counterpart='he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *feminism-conservatism*\n",
    "\n",
    "$\\vec{\\text{she}} - \\vec{\\text{feminism}} \\approx \\vec{\\text{he}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feminism', 0.6571188),\n",
       " ('liberalism', 0.5454636812210083),\n",
       " ('liberationist', 0.529727578163147),\n",
       " ('conservatism', 0.5211994051933289),\n",
       " ('neoconservativism', 0.5119233131408691),\n",
       " ('anarchism', 0.5116571187973022),\n",
       " ('leftism', 0.5088927745819092),\n",
       " ('intellectualism', 0.5075400471687317),\n",
       " ('Neo_conservatism', 0.5073385238647461),\n",
       " ('postmodernism', 0.4998423755168915),\n",
       " ('Hegelian_dialectic', 0.49135786294937134)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='feminism', counterpart='he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{\\text{she}} - \\vec{\\text{feminism}} \\approx \\vec{\\text{she}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feminism', 1.0000001),\n",
       " ('feminist', 0.8082043528556824),\n",
       " ('feminists', 0.7639337778091431),\n",
       " ('feminist_movement', 0.7451367974281311),\n",
       " ('Feminism', 0.7141621112823486),\n",
       " ('radical_feminism', 0.6737356781959534),\n",
       " ('radical_feminist', 0.6637553572654724),\n",
       " ('Betty_Freidan', 0.6418492794036865),\n",
       " ('feminisms', 0.6368929743766785),\n",
       " ('womanist', 0.6334027051925659),\n",
       " ('women_libbers', 0.6325899958610535)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='feminism', counterpart='she')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *lovely-brilliant*\n",
    "\n",
    "$\\vec{\\text{she}} - \\vec{\\text{lovely}} \\approx \\vec{\\text{he}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lovely', 0.69995606),\n",
       " ('magnificent', 0.6248228549957275),\n",
       " ('marvelous', 0.6054928302764893),\n",
       " ('splendid', 0.5995590686798096),\n",
       " ('nice', 0.5869458913803101),\n",
       " ('fantastic', 0.5587064027786255),\n",
       " ('delightful', 0.5561120510101318),\n",
       " ('terrific', 0.5524159669876099),\n",
       " ('wonderful', 0.5481390953063965),\n",
       " ('brilliant', 0.5460425615310669),\n",
       " ('beautiful', 0.545063316822052)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='lovely', counterpart='he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lovely', 1.0),\n",
       " ('beautiful', 0.8106936812400818),\n",
       " ('gorgeous', 0.8014094233512878),\n",
       " ('delightful', 0.7586833238601685),\n",
       " ('wonderful', 0.7320095896720886),\n",
       " ('fabulous', 0.712957501411438),\n",
       " ('marvelous', 0.6729508638381958),\n",
       " ('nice', 0.6676310300827026),\n",
       " ('charming', 0.6509542465209961),\n",
       " ('magnificent', 0.650709867477417),\n",
       " ('splendid', 0.6399756669998169)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='lovely', counterpart='she')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *charming-affable*\n",
    "$\\vec{\\text{she}} - \\vec{\\text{charming}} \\approx \\vec{\\text{he}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('charming', 0.72905624),\n",
       " ('genial', 0.6243418455123901),\n",
       " ('affable', 0.6227308511734009),\n",
       " ('likeable', 0.6079907417297363),\n",
       " ('amiable', 0.5914740562438965),\n",
       " ('likable', 0.5785181522369385),\n",
       " ('unassuming', 0.5687059760093689),\n",
       " ('urbane', 0.5588454604148865),\n",
       " ('jovial', 0.5488089323043823),\n",
       " ('unpretentious', 0.5404201745986938),\n",
       " ('suave', 0.5303424596786499)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='charming', counterpart='he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *cosmetics-pharmaceuticals*\n",
    "$\\vec{\\text{she}} - \\vec{\\text{cosmetics}} \\approx \\vec{\\text{he}} - \\vec{\\text{??}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cosmetics', 0.65411854),\n",
       " ('cosmetics_perfumes', 0.499711811542511),\n",
       " ('skin_creams', 0.4964258670806885),\n",
       " ('haircare', 0.49254095554351807),\n",
       " ('pharmaceuticals', 0.4858422577381134),\n",
       " ('cosmetic', 0.4819394648075104),\n",
       " ('maker_Shiseido', 0.46269580721855164),\n",
       " ('Clarins_SA', 0.4574735760688782),\n",
       " ('creams_soaps', 0.45667964220046997),\n",
       " ('skincare', 0.45505884289741516),\n",
       " ('perfumes', 0.4498598575592041)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='cosmetics', counterpart='he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cosmetics', 1.0),\n",
       " ('Cosmetics', 0.7287227511405945),\n",
       " ('skincare', 0.6951978206634521),\n",
       " ('skin_creams', 0.6833450794219971),\n",
       " ('cosmetic', 0.6811989545822144),\n",
       " ('maker_Shiseido', 0.6809768676757812),\n",
       " ('haircare', 0.6790057420730591),\n",
       " ('perfumes', 0.6575888991355896),\n",
       " ('creams_soaps', 0.6494388580322266),\n",
       " ('bodycare', 0.6485159397125244),\n",
       " ('perfume', 0.6439985036849976)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_full_list(subj='she', obj='cosmetics', counterpart='she')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carpenter', 0.5112387537956238),\n",
       " ('tinkerer', 0.47657930850982666),\n",
       " ('machinist', 0.47604185342788696),\n",
       " ('mechanical_engineer', 0.4732446074485779),\n",
       " ('lifelong_resident', 0.471080482006073),\n",
       " ('avid_fisherman', 0.4508781135082245),\n",
       " ('laborer', 0.4494982957839966),\n",
       " ('retired', 0.4469570219516754),\n",
       " ('businessman', 0.4450863301753998),\n",
       " ('retired_schoolteacher', 0.44203752279281616)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['he', 'homemaker'], negative=['she'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12233140540741333\n",
      "0.36942589567514705\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('he', 'nurse'))\n",
    "print(model.similarity('she', 'nurse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doctor', 0.5559605360031128),\n",
       " ('medic', 0.5425376892089844),\n",
       " ('physician', 0.5394270420074463),\n",
       " ('x_ray_technician', 0.5355567932128906),\n",
       " ('surgeon', 0.516014575958252),\n",
       " ('nurses', 0.49741458892822266),\n",
       " ('paramedic', 0.4924110770225525),\n",
       " ('anesthetist', 0.4886544942855835),\n",
       " ('patient', 0.46544167399406433),\n",
       " ('doctors', 0.4639861583709717)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nurse_list = model.most_similar(positive=['he', 'nurse'], negative=['she'])\n",
    "nurse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nurse: 0.6655272245407104\n",
      "doctor: 0.5559605360031128\n",
      "medic: 0.5425377488136292\n",
      "physician: 0.5394271612167358\n",
      "x_ray_technician: 0.5355568528175354\n",
      "surgeon: 0.5160146951675415\n",
      "nurses: 0.49741464853286743\n",
      "paramedic: 0.4924110770225525\n",
      "anesthetist: 0.4886545240879059\n",
      "patient: 0.46544164419174194\n",
      "doctors: 0.46398621797561646\n"
     ]
    }
   ],
   "source": [
    "for x1, x2 in [('nurse', 0)] + nurse_list:\n",
    "    vec1 = (model.get_vector('he') + model.get_vector('nurse') - model.get_vector('she')).reshape(1, 300)\n",
    "    vec2 = model.get_vector(x1).reshape(1, 300)\n",
    "    print('{}: {}'.format(x1, cosine_similarity(vec1, vec2)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carpenter', 0.5112387537956238),\n",
       " ('tinkerer', 0.47657930850982666),\n",
       " ('machinist', 0.47604185342788696),\n",
       " ('mechanical_engineer', 0.4732446074485779),\n",
       " ('lifelong_resident', 0.471080482006073),\n",
       " ('avid_fisherman', 0.4508781135082245),\n",
       " ('laborer', 0.4494982957839966),\n",
       " ('retired', 0.4469570219516754),\n",
       " ('businessman', 0.4450863301753998),\n",
       " ('retired_schoolteacher', 0.44203752279281616)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homemaker_list = model.most_similar(positive=['he', 'homemaker'], negative=['she'])\n",
    "homemaker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homemaker: 0.657961905002594\n",
      "carpenter: 0.5112388730049133\n",
      "tinkerer: 0.47657933831214905\n",
      "machinist: 0.47604191303253174\n",
      "mechanical_engineer: 0.4732445478439331\n",
      "lifelong_resident: 0.4710805118083954\n",
      "avid_fisherman: 0.4508780837059021\n",
      "laborer: 0.4494982659816742\n",
      "retired: 0.4469570815563202\n",
      "businessman: 0.44508635997772217\n",
      "retired_schoolteacher: 0.4420374631881714\n"
     ]
    }
   ],
   "source": [
    "for x1, x2 in [('homemaker', 0)] + homemaker_list:\n",
    "    vec1 = (model.get_vector('he') + model.get_vector('homemaker') - model.get_vector('she')).reshape(1, 300)\n",
    "    vec2 = model.get_vector(x1).reshape(1, 300)\n",
    "    print('{}: {}'.format(x1, cosine_similarity(vec1, vec2)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nurse: 0.8459365367889404\n",
      "doctor: 0.6182307600975037\n",
      "medic: 0.6041296720504761\n",
      "physician: 0.6366410255432129\n",
      "x_ray_technician: 0.6145442128181458\n",
      "surgeon: 0.5559583902359009\n",
      "nurses: 0.6771997213363647\n",
      "paramedic: 0.579715371131897\n",
      "anesthetist: 0.5961609482765198\n",
      "patient: 0.568503201007843\n",
      "doctors: 0.5336419343948364\n"
     ]
    }
   ],
   "source": [
    "for x1, x2 in [('nurse', 0)] + nurse_list:\n",
    "    vec1 = (model.get_vector('nurse') - model.get_vector('she')).reshape(1, 300)\n",
    "    vec2 = (model.get_vector(x1) - model.get_vector('he')).reshape(1, 300)\n",
    "    print('{}: {}'.format(x1, cosine_similarity(vec1, vec2)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nurse: 0.8459365367889404\n",
      "doctor: 0.6182307600975037\n",
      "medic: 0.6041296720504761\n",
      "physician: 0.6366410255432129\n",
      "x_ray_technician: 0.6145442128181458\n",
      "surgeon: 0.5559583902359009\n",
      "nurses: 0.6771997213363647\n",
      "paramedic: 0.579715371131897\n",
      "anesthetist: 0.5961609482765198\n",
      "patient: 0.568503201007843\n",
      "doctors: 0.5336419343948364\n"
     ]
    }
   ],
   "source": [
    "for x1, x2 in [('nurse', 0)] + nurse_list:\n",
    "    vec1 = (model.word_vec('nurse') - model.word_vec('she')).reshape(1, 300)\n",
    "    vec2 = (model.word_vec(x1) - model.word_vec('he')).reshape(1, 300)\n",
    "    print('{}: {}'.format(x1, cosine_similarity(vec1, vec2)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Class to contain vectors and vocab for word2vec model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False)\n",
      " |      Return a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `keras.layers.Embedding`\n",
      " |          Embedding layer\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec :  int\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Note that the information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str\n",
      " |              Optional file path to the vocabulary.Word counts are read from `fvocab` filename,\n",
      " |              if set (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that\n",
      " |          encoding in `encoding`.\n",
      " |      unicode_errors : str\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : :class: `numpy.float*`\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such\n",
      " |          as np.float16) to save memory. (Such types may result in much slower bulk operations\n",
      " |          or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `~gensim.models.word2vec.Wod2Vec`\n",
      " |          Returns the loaded model as an instance of :class: `~gensim.models.word2vec.Wod2Vec`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7f72f257a840>, case_insensitive=True)\n",
      " |      Compute accuracy of the model. `questions` is a filename where lines are\n",
      " |      4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |      See questions-words.txt in\n",
      " |      https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip\n",
      " |      for an example.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Use `restrict_vocab` to ignore all questions containing a word not in the first `restrict_vocab`\n",
      " |      words (default 30,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      " |      In case `case_insensitive` is True, the first `restrict_vocab` words are taken first, and then\n",
      " |      case normalization is performed.\n",
      " |      \n",
      " |      Use `case_insensitive` to convert all words in questions and vocab to their uppercase form before\n",
      " |      evaluating the accuracy (default True). Useful in case of case-mismatch between training tokens\n",
      " |      and question words. In case of multiple case variants of a single word, the vector for the first\n",
      " |      occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> trained_model.distance('woman', 'man')\n",
      " |      0.34\n",
      " |      \n",
      " |      >>> trained_model.distance('woman', 'woman')\n",
      " |      0.0\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : str or numpy.array\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      \n",
      " |      other_words : iterable(str) or None\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`,\n",
      " |          in the same order as `other_words`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Raises KeyError if either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : :obj: `list` of :obj: `str`\n",
      " |          List of words\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      >>> trained_model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
      " |      'cereal'\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments. `pairs` is a filename of a dataset where\n",
      " |      lines are 3-tuples, each consisting of a word pair and a similarity value, separated by `delimiter`.\n",
      " |      An example dataset is included in Gensim (test/test_data/wordsim353.tsv). More datasets can be found at\n",
      " |      http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html or https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      The model is evaluated using Pearson correlation coefficient and Spearman rank-order correlation coefficient\n",
      " |      between the similarities from the dataset and the similarities produced by the model itself.\n",
      " |      The results are printed to log and returned as a triple (pearson, spearman, ratio of pairs with unknown words).\n",
      " |      \n",
      " |      Use `restrict_vocab` to ignore all word pairs containing a word not in the first `restrict_vocab`\n",
      " |      words (default 300,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      " |      If `case_insensitive` is True, the first `restrict_vocab` words are taken, and then case normalization\n",
      " |      is performed.\n",
      " |      \n",
      " |      Use `case_insensitive` to convert all words in the pairs and vocab to their uppercase form before\n",
      " |      evaluating the model (default True). Useful when you expect case-mismatch between training tokens\n",
      " |      and words pairs in the dataset. If there are multiple case variants of a single word, the vector for the first\n",
      " |      occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Use `dummy4unknown=True` to produce zero-valued similarities for pairs with out-of-vocabulary words.\n",
      " |      Otherwise (default False), these pairs are skipped entirely.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Accept a single entity as input, specified by string tag.\n",
      " |      Returns the entity's representations in vector space, as a 1D numpy array.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      If `replace` is set, forget the original vectors and only keep the normalized\n",
      " |      ones = saves lots of memory!\n",
      " |      \n",
      " |      Note that you **cannot continue training** after doing a replace. The model becomes\n",
      " |      effectively read-only = you can call `most_similar`, `similarity` etc., but not `train`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words. Positive words contribute positively towards the\n",
      " |      similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : :obj: `list` of :obj: `str`\n",
      " |          List of words that contribute positively.\n",
      " |      negative : :obj: `list` of :obj: `str`\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int\n",
      " |          Number of top-N similar words to return.\n",
      " |      restrict_vocab : int\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `list` of :obj: `tuple`\n",
      " |          Returns a list of tuples (word, similarity)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> trained_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
      " |      [('queen', 0.50882536), ...]\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective\n",
      " |      proposed by Omer Levy and Yoav Goldberg. Positive words still contribute\n",
      " |      positively towards the similarity, negative words negatively, but with less\n",
      " |      susceptibility to one large distance dominating the calculation.\n",
      " |      \n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively â€“ a potentially sensible but untested extension of the method. (With\n",
      " |      a single positive example, rankings will be the same as in the default most_similar.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london'])\n",
      " |        [(u'iraq', 0.8488819003105164), ...]\n",
      " |      \n",
      " |      .. Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations, 2014.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> trained_model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\n",
      " |      0.61540466561049689\n",
      " |      \n",
      " |      >>> trained_model.n_similarity(['restaurant', 'japanese'], ['japanese', 'restaurant'])\n",
      " |      1.0000000000000004\n",
      " |      \n",
      " |      >>> trained_model.n_similarity(['sushi'], ['restaurant']) == trained_model.similarity('sushi', 'restaurant')\n",
      " |      True\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Saves the keyedvectors. This saved model can be loaded again using\n",
      " |      :func:`~gensim.models.*2vec.*2VecKeyedVectors.load` which supports\n",
      " |      operations on trained word vectors like `most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          vector from which similarities are to be computed.\n",
      " |          expected shape (dim,)\n",
      " |      topn : int\n",
      " |          Number of top-N similar words to return. If topn is False, similar_by_vector returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `list` of :obj: `tuple`\n",
      " |          Returns a list of tuples (word, similarity)\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int\n",
      " |          Number of top-N similar words to return. If topn is False, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `list` of :obj: `tuple`\n",
      " |          Returns a list of tuples (word, similarity)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.similar_by_word('graph')\n",
      " |        [('user', 0.9999163150787354), ...]\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> trained_model.similarity('woman', 'man')\n",
      " |      0.73723527\n",
      " |      \n",
      " |      >>> trained_model.similarity('woman', 'woman')\n",
      " |      1.0\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Constructs a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      Constructs a a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies a mapping between words and the indices of rows and columns\n",
      " |          of the resulting term similarity matrix.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel`, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The rows\n",
      " |          of the term similarity matrix will be build in an increasing order of importance of terms,\n",
      " |          or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only pairs of words whose embeddings are more similar than `threshold` are considered\n",
      " |          when building the sparse term similarity matrix.\n",
      " |      exponent : float, optional\n",
      " |          The exponent applied to the similarity between two word embeddings when building the term similarity matrix.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single row or column\n",
      " |          of the term similarity matrix. Setting `nonzero_limit` to a constant ensures that the\n",
      " |          time complexity of computing the Soft Cosine Measure will be linear in the document\n",
      " |          length rather than quadratic.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`__.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents. When using this\n",
      " |      code, please consider citing the following papers:\n",
      " |      \n",
      " |      .. Ofir Pele and Michael Werman, \"A linear time histogram metric for improved SIFT matching\".\n",
      " |      .. Ofir Pele and Michael Werman, \"Fast and robust earth mover's distances\".\n",
      " |      .. Matt Kusner et al. \"From Word Embeddings To Document Distances\".\n",
      " |      \n",
      " |      Note that if one of the documents have no words that exist in the\n",
      " |      Word2Vec vocab, `float('inf')` (i.e. infinity) will be returned.\n",
      " |      \n",
      " |      This method only works if `pyemd` is installed (can be installed via pip, but requires a C compiler).\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> # Train word2vec model.\n",
      " |          >>> model = Word2Vec(sentences)\n",
      " |      \n",
      " |          >>> # Some sentences to test.\n",
      " |          >>> sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
      " |          >>> sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
      " |      \n",
      " |          >>> # Remove their stopwords.\n",
      " |          >>> from nltk.corpus import stopwords\n",
      " |          >>> stopwords = nltk.corpus.stopwords.words('english')\n",
      " |          >>> sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
      " |          >>> sentence_president = [w for w in sentence_president if w not in stopwords]\n",
      " |      \n",
      " |          >>> # Compute WMD.\n",
      " |          >>> distance = model.wmdistance(sentence_obama, sentence_president)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Accept a single word as input.\n",
      " |      Returns the word's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      If `use_norm` is True, returns the normalized word vector.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> trained_model['office']\n",
      " |      array([ -1.40128313e-02, ...])\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Returns all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> model.words_closer_than('carnivore', 'mammal')\n",
      " |      ['dog', 'canine']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Return cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.array\n",
      " |          vector from which similarities are to be computed.\n",
      " |          expected shape (dim,)\n",
      " |      vectors_all : numpy.array\n",
      " |          for each row in vectors_all, distance from vector_1 is computed.\n",
      " |          expected shape (num_vectors, dim)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `numpy.array`\n",
      " |          Contains cosine distance between vector_1 and each row in vectors_all.\n",
      " |          shape (num_vectors,)\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Accept a single entity (string tag) or list of entities as input.\n",
      " |      \n",
      " |      If a single string or int, return designated tag's vector\n",
      " |      representation, as a 1D numpy array.\n",
      " |      \n",
      " |      If a list, return designated tags' vector representations as a\n",
      " |      2D numpy array: #tags x #vector_size.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Returns all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Return the entity from entities_list most similar to entity1.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load a previously saved object (using :meth:`~gensim.utils.SaveLoad.save`) from file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      IOError\n",
      " |          When methods are called on instance (should be called from class).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('registered_nurse', 0.7375059127807617),\n",
       " ('nurse_practitioner', 0.6650707721710205),\n",
       " ('midwife', 0.6506887078285217),\n",
       " ('nurses', 0.6448696851730347),\n",
       " ('nurse_midwife', 0.6239830255508423),\n",
       " ('birth_doula', 0.5852459669113159),\n",
       " ('neonatal_nurse', 0.5670715570449829),\n",
       " ('dental_hygienist', 0.5668443441390991),\n",
       " ('lactation_consultant', 0.5667990446090698),\n",
       " ('respiratory_therapist', 0.5652168989181519)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'nurse'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Class to contain vectors and vocab for word2vec model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False)\n",
      " |      Return a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `keras.layers.Embedding`\n",
      " |          Embedding layer\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec :  int\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Note that the information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str\n",
      " |              Optional file path to the vocabulary.Word counts are read from `fvocab` filename,\n",
      " |              if set (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that\n",
      " |          encoding in `encoding`.\n",
      " |      unicode_errors : str\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : :class: `numpy.float*`\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such\n",
      " |          as np.float16) to save memory. (Such types may result in much slower bulk operations\n",
      " |          or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `~gensim.models.word2vec.Wod2Vec`\n",
      " |          Returns the loaded model as an instance of :class: `~gensim.models.word2vec.Wod2Vec`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7f72f257a840>, case_insensitive=True)\n",
      " |      Compute accuracy of the model. `questions` is a filename where lines are\n",
      " |      4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |      See questions-words.txt in\n",
      " |      https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip\n",
      " |      for an example.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Use `restrict_vocab` to ignore all questions containing a word not in the first `restrict_vocab`\n",
      " |      words (default 30,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      " |      In case `case_insensitive` is True, the first `restrict_vocab` words are taken first, and then\n",
      " |      case normalization is performed.\n",
      " |      \n",
      " |      Use `case_insensitive` to convert all words in questions and vocab to their uppercase form before\n",
      " |      evaluating the accuracy (default True). Useful in case of case-mismatch between training tokens\n",
      " |      and question words. In case of multiple case variants of a single word, the vector for the first\n",
      " |      occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> trained_model.distance('woman', 'man')\n",
      " |      0.34\n",
      " |      \n",
      " |      >>> trained_model.distance('woman', 'woman')\n",
      " |      0.0\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : str or numpy.array\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      \n",
      " |      other_words : iterable(str) or None\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`,\n",
      " |          in the same order as `other_words`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Raises KeyError if either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : :obj: `list` of :obj: `str`\n",
      " |          List of words\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      >>> trained_model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
      " |      'cereal'\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments. `pairs` is a filename of a dataset where\n",
      " |      lines are 3-tuples, each consisting of a word pair and a similarity value, separated by `delimiter`.\n",
      " |      An example dataset is included in Gensim (test/test_data/wordsim353.tsv). More datasets can be found at\n",
      " |      http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html or https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      The model is evaluated using Pearson correlation coefficient and Spearman rank-order correlation coefficient\n",
      " |      between the similarities from the dataset and the similarities produced by the model itself.\n",
      " |      The results are printed to log and returned as a triple (pearson, spearman, ratio of pairs with unknown words).\n",
      " |      \n",
      " |      Use `restrict_vocab` to ignore all word pairs containing a word not in the first `restrict_vocab`\n",
      " |      words (default 300,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      " |      If `case_insensitive` is True, the first `restrict_vocab` words are taken, and then case normalization\n",
      " |      is performed.\n",
      " |      \n",
      " |      Use `case_insensitive` to convert all words in the pairs and vocab to their uppercase form before\n",
      " |      evaluating the model (default True). Useful when you expect case-mismatch between training tokens\n",
      " |      and words pairs in the dataset. If there are multiple case variants of a single word, the vector for the first\n",
      " |      occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Use `dummy4unknown=True` to produce zero-valued similarities for pairs with out-of-vocabulary words.\n",
      " |      Otherwise (default False), these pairs are skipped entirely.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Accept a single entity as input, specified by string tag.\n",
      " |      Returns the entity's representations in vector space, as a 1D numpy array.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      If `replace` is set, forget the original vectors and only keep the normalized\n",
      " |      ones = saves lots of memory!\n",
      " |      \n",
      " |      Note that you **cannot continue training** after doing a replace. The model becomes\n",
      " |      effectively read-only = you can call `most_similar`, `similarity` etc., but not `train`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words. Positive words contribute positively towards the\n",
      " |      similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : :obj: `list` of :obj: `str`\n",
      " |          List of words that contribute positively.\n",
      " |      negative : :obj: `list` of :obj: `str`\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int\n",
      " |          Number of top-N similar words to return.\n",
      " |      restrict_vocab : int\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `list` of :obj: `tuple`\n",
      " |          Returns a list of tuples (word, similarity)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> trained_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
      " |      [('queen', 0.50882536), ...]\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective\n",
      " |      proposed by Omer Levy and Yoav Goldberg. Positive words still contribute\n",
      " |      positively towards the similarity, negative words negatively, but with less\n",
      " |      susceptibility to one large distance dominating the calculation.\n",
      " |      \n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively â€“ a potentially sensible but untested extension of the method. (With\n",
      " |      a single positive example, rankings will be the same as in the default most_similar.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london'])\n",
      " |        [(u'iraq', 0.8488819003105164), ...]\n",
      " |      \n",
      " |      .. Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations, 2014.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> trained_model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\n",
      " |      0.61540466561049689\n",
      " |      \n",
      " |      >>> trained_model.n_similarity(['restaurant', 'japanese'], ['japanese', 'restaurant'])\n",
      " |      1.0000000000000004\n",
      " |      \n",
      " |      >>> trained_model.n_similarity(['sushi'], ['restaurant']) == trained_model.similarity('sushi', 'restaurant')\n",
      " |      True\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Saves the keyedvectors. This saved model can be loaded again using\n",
      " |      :func:`~gensim.models.*2vec.*2VecKeyedVectors.load` which supports\n",
      " |      operations on trained word vectors like `most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          vector from which similarities are to be computed.\n",
      " |          expected shape (dim,)\n",
      " |      topn : int\n",
      " |          Number of top-N similar words to return. If topn is False, similar_by_vector returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `list` of :obj: `tuple`\n",
      " |          Returns a list of tuples (word, similarity)\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int\n",
      " |          Number of top-N similar words to return. If topn is False, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `list` of :obj: `tuple`\n",
      " |          Returns a list of tuples (word, similarity)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.similar_by_word('graph')\n",
      " |        [('user', 0.9999163150787354), ...]\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> trained_model.similarity('woman', 'man')\n",
      " |      0.73723527\n",
      " |      \n",
      " |      >>> trained_model.similarity('woman', 'woman')\n",
      " |      1.0\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Constructs a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      Constructs a a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies a mapping between words and the indices of rows and columns\n",
      " |          of the resulting term similarity matrix.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel`, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The rows\n",
      " |          of the term similarity matrix will be build in an increasing order of importance of terms,\n",
      " |          or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only pairs of words whose embeddings are more similar than `threshold` are considered\n",
      " |          when building the sparse term similarity matrix.\n",
      " |      exponent : float, optional\n",
      " |          The exponent applied to the similarity between two word embeddings when building the term similarity matrix.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single row or column\n",
      " |          of the term similarity matrix. Setting `nonzero_limit` to a constant ensures that the\n",
      " |          time complexity of computing the Soft Cosine Measure will be linear in the document\n",
      " |          length rather than quadratic.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`__.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents. When using this\n",
      " |      code, please consider citing the following papers:\n",
      " |      \n",
      " |      .. Ofir Pele and Michael Werman, \"A linear time histogram metric for improved SIFT matching\".\n",
      " |      .. Ofir Pele and Michael Werman, \"Fast and robust earth mover's distances\".\n",
      " |      .. Matt Kusner et al. \"From Word Embeddings To Document Distances\".\n",
      " |      \n",
      " |      Note that if one of the documents have no words that exist in the\n",
      " |      Word2Vec vocab, `float('inf')` (i.e. infinity) will be returned.\n",
      " |      \n",
      " |      This method only works if `pyemd` is installed (can be installed via pip, but requires a C compiler).\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> # Train word2vec model.\n",
      " |          >>> model = Word2Vec(sentences)\n",
      " |      \n",
      " |          >>> # Some sentences to test.\n",
      " |          >>> sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
      " |          >>> sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
      " |      \n",
      " |          >>> # Remove their stopwords.\n",
      " |          >>> from nltk.corpus import stopwords\n",
      " |          >>> stopwords = nltk.corpus.stopwords.words('english')\n",
      " |          >>> sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
      " |          >>> sentence_president = [w for w in sentence_president if w not in stopwords]\n",
      " |      \n",
      " |          >>> # Compute WMD.\n",
      " |          >>> distance = model.wmdistance(sentence_obama, sentence_president)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Accept a single word as input.\n",
      " |      Returns the word's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      If `use_norm` is True, returns the normalized word vector.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> trained_model['office']\n",
      " |      array([ -1.40128313e-02, ...])\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Returns all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> model.words_closer_than('carnivore', 'mammal')\n",
      " |      ['dog', 'canine']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Return cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.array\n",
      " |          vector from which similarities are to be computed.\n",
      " |          expected shape (dim,)\n",
      " |      vectors_all : numpy.array\n",
      " |          for each row in vectors_all, distance from vector_1 is computed.\n",
      " |          expected shape (num_vectors, dim)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `numpy.array`\n",
      " |          Contains cosine distance between vector_1 and each row in vectors_all.\n",
      " |          shape (num_vectors,)\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Accept a single entity (string tag) or list of entities as input.\n",
      " |      \n",
      " |      If a single string or int, return designated tag's vector\n",
      " |      representation, as a 1D numpy array.\n",
      " |      \n",
      " |      If a list, return designated tags' vector representations as a\n",
      " |      2D numpy array: #tags x #vector_size.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Returns all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Return the entity from entities_list most similar to entity1.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load a previously saved object (using :meth:`~gensim.utils.SaveLoad.save`) from file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      IOError\n",
      " |          When methods are called on instance (should be called from class).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     in the case in word2vec, is vec(queen). Note that the input\n",
    "    \n",
    "#     check_full_list(counterpart='she', obj='king', subj='he')\n",
    "    \n",
    "#     would return the results from\n",
    "    \n",
    "#     model.most_similar(positive=[counterpart, obj], negative=[subj])\n",
    "    \n",
    "#     together with the cosine similarity between\n",
    "    \n",
    "#     vec(counterpart) - vec(subj) + vec(obj)\n",
    "    \n",
    "#     and\n",
    "    \n",
    "#     vec(obj)\n",
    "    \n",
    "#     for comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
